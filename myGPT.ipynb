{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgTZluaMzu9cdHtsNUYP3q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"y4n909t1wulH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689149093298,"user_tz":-480,"elapsed":3043,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"927a07df-c6aa-4bd8-a6d5-d0f2c6152c43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["cd \"/content/gdrive/My Drive/Sequential_model/my_GPT\""],"metadata":{"id":"QS89fN7lw6Ba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689149093300,"user_tz":-480,"elapsed":14,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"5cdd8156-7286-4dfb-c231-d4fd6b582d41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Sequential_model/my_GPT\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","import numpy as np\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"OgOt7Ptt4DnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = []\n","with open('math_dataset.txt', 'r') as file:\n","    for line in file:\n","        data.append(line)"],"metadata":{"id":"xcRYyF3pw6Qd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[:6]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r80dsm1k34aM","executionInfo":{"status":"ok","timestamp":1689149097633,"user_tz":-480,"elapsed":8,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"800dbb9c-f9ad-40a9-8958-872269fa3aec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Q: John has 5 apples, and he gives 2 apples to his friend. How many apples does John have left?\\n',\n"," 'A: John has 3 apples left.\\n',\n"," '\\n',\n"," 'Q: Sara bought a shirt for $20 and a pair of pants for $30. How much did she spend in total?\\n',\n"," 'A: Sara spent a total of $50.\\n',\n"," '\\n']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["PAD_token = 0\n","SOS_token = 1\n","EOS_token = 2\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n","        for i in range(1000):\n","            special_tokens.append(str(i))\n","        self.word2index = {}#{\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\": 2}\n","        self.word2count = {}#{\"<PAD>\":1, \"<SOS>\":1, \"<EOS>\": 1}\n","        self.index2word = {}#{0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n","        self.n_words = 0#3  # Count PAD, SOS and EOS\n","        self.max_len = 0\n","\n","        for t in special_tokens:\n","            self.word2index[t] = self.n_words\n","            self.word2count[t] = 1\n","            self.index2word[self.n_words] = t\n","            self.n_words += 1\n","\n","    def addSentence(self, sentence):\n","        updated_word = self.splitWord(sentence)\n","\n","        if len(updated_word) > self.max_len:\n","            self.max_len = len(updated_word)\n","        for word in updated_word:\n","            self.addWord(word)\n","\n","    def splitWord(self, sentence):\n","        #======= add space for certain character ==========\n","        addSpaceBefore = [',', ':', '.', '?', '/']\n","        str1 = \"\"\n","        for char in sentence:\n","            if char in addSpaceBefore:\n","                str1 += \" \" + char\n","            else:\n","                str1 += char\n","\n","        addSpaceAfter = ['.', '/', '$']\n","        str2 = \"\"\n","        for char in str1:\n","            if char in addSpaceAfter:\n","                str2 += char + \" \"\n","            else:\n","                str2 += char\n","\n","        return str2.split()\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"metadata":{"id":"X0NlnGrUw6co"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q = \"Q: Sara bought a shirt for $20.00 and a pair of pants for $30. How much did she spend in total?\"\n","en = Lang(\"EN\")\n","print(en.splitWord(q))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"axLBukJGEXL4","executionInfo":{"status":"ok","timestamp":1689149098301,"user_tz":-480,"elapsed":29,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"d6225adb-2362-4715-9058-04148228fb89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Q', ':', 'Sara', 'bought', 'a', 'shirt', 'for', '$', '20', '.', '00', 'and', 'a', 'pair', 'of', 'pants', 'for', '$', '30', '.', 'How', 'much', 'did', 'she', 'spend', 'in', 'total', '?']\n"]}]},{"cell_type":"code","source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in lang.splitWord(sentence)]\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    #indexes.insert(0, SOS_token)\n","    indexes.append(lang.word2index[\"<EOS>\"])\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1) #(1,t)"],"metadata":{"id":"-VoACZO5vZgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepareData():\n","    qa_pairs = []\n","    questions = []\n","    answers = []\n","    for i in range(0, len(data), 3):\n","        questions.append(data[i].replace(\"\\n\", \"\"))\n","        answers.append(data[i+1].replace(\"\\n\", \"\"))\n","        qa_pairs.append(data[i].replace(\"\\n\", \"\") + \" \" + data[i+1].replace(\"\\n\", \"\"))\n","    return qa_pairs, questions, answers\n","\n","qa_pairs, questions, answers = prepareData()\n","print(qa_pairs[:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rUvpjCbvZbu","executionInfo":{"status":"ok","timestamp":1689152222916,"user_tz":-480,"elapsed":16,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"7d1db5db-2174-46eb-f05e-fdce5dcbb992"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Q: John has 5 apples, and he gives 2 apples to his friend. How many apples does John have left? A: John has 3 apples left.', 'Q: Sara bought a shirt for $20 and a pair of pants for $30. How much did she spend in total? A: Sara spent a total of $50.']\n"]}]},{"cell_type":"code","source":["for i in range(len(qa_pairs)):\n","    en.addSentence(qa_pairs[i])\n","print(en.n_words)"],"metadata":{"id":"4ThgJs9k5qIP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689149098301,"user_tz":-480,"elapsed":22,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"9e05819e-8ead-42b7-9102-47b2388e9e11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1255\n"]}]},{"cell_type":"code","source":["#tensorFromSentence(en, qa_pairs[0])"],"metadata":{"id":"b7izdPwS4TNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(en.splitWord(qa_pairs[0]))\n","print(len(en.splitWord(qa_pairs[0])))\n","print(indexesFromSentence(en, qa_pairs[0]))\n","print(len(indexesFromSentence(en, qa_pairs[0])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUdojM-AvZki","executionInfo":{"status":"ok","timestamp":1689149098302,"user_tz":-480,"elapsed":19,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"2a332d89-b202-49a6-bf58-5b9f2b5a9a80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Q', ':', 'John', 'has', '5', 'apples', ',', 'and', 'he', 'gives', '2', 'apples', 'to', 'his', 'friend', '.', 'How', 'many', 'apples', 'does', 'John', 'have', 'left', '?', 'A', ':', 'John', 'has', '3', 'apples', 'left', '.']\n","32\n","[1004, 1005, 1006, 1007, 9, 1008, 1009, 1010, 1011, 1012, 6, 1008, 1013, 1014, 1015, 1016, 1017, 1018, 1008, 1019, 1006, 1020, 1021, 1022, 1023, 1005, 1006, 1007, 7, 1008, 1021, 1016]\n","32\n"]}]},{"cell_type":"code","source":["SOS_token = en.word2index['<SOS>']\n","EOS_token = en.word2index['<EOS>']\n","MAX_LENGTH = en.max_len #max sentence length\n","\n","def get_dataloader(batch_size):\n","\n","    qa_pairs, _, _ = prepareData() #input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n","    \"\"\"load all data (indexs) to train_data\"\"\"\n","    n = len(qa_pairs)\n","    input_ids = np.zeros((n, MAX_LENGTH+2), dtype=np.int32) # +<SOS> <EOS>\n","    target_ids = np.zeros((n, MAX_LENGTH+2), dtype=np.int32)\n","\n","    for idx, x in enumerate(qa_pairs):\n","        #Sentence to indexs\n","        inp_ids = indexesFromSentence(en, x)\n","        tgt_ids = indexesFromSentence(en, x)\n","        #add <SOS> index\n","        inp_ids.insert(0, SOS_token)\n","        tgt_ids.insert(0, SOS_token)\n","        #add <EOS> index\n","        inp_ids.append(EOS_token)\n","        tgt_ids.append(EOS_token)\n","        #remove last token of inp, remove first token of tgt\n","        inp_ids.remove(inp_ids[-1])\n","        tgt_ids.remove(tgt_ids[0])\n","        input_ids[idx, :len(inp_ids)] = inp_ids #inp_ids =          <SOS> <This> <is> <example> <sentence> removed <PAD> <PAD> .... <PAD>\n","        target_ids[idx, :len(tgt_ids)] = tgt_ids #inp_ids = removed <This> <is> <example> <sentence> <EOS> <PAD> <PAD> .... <PAD>. when Y is <PAD> loss will not be computed\n","\n","    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n","                               torch.LongTensor(target_ids).to(device))\n","\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","    return train_dataloader"],"metadata":{"id":"SRrZwJhziv6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataloader = get_dataloader(2)\n","for inp, tgt in train_dataloader:\n","    print(inp.shape)\n","    print(tgt.shape)\n","    indexs = tgt[0].tolist()\n","    print(\" \".join([en.index2word[idx] for idx in indexs]))\n","\n","    for b in range(1): # batch dimension\n","        for t in range(len(tgt[0].tolist())-1): # time dimension\n","            context = inp[b, :t+1]\n","            target = tgt[b,t]\n","            print(f\"when input is {context.tolist()} the target: {target}\")\n","\n","    break\n","\n","    for i in range(len(indexs)-1):\n","        print(f\"When input, x is {indexs[:1+i]}, target, Y is {indexs[i+1]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0hcHIammmtK","executionInfo":{"status":"ok","timestamp":1689149098303,"user_tz":-480,"elapsed":17,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"376e1829-c030-46b2-f8b5-979527b56e86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 52])\n","torch.Size([2, 52])\n","Q : Peter has 4 times as many marbles as Jack . If Jack has 8 marbles , how many marbles does Peter have ? A : Peter has 32 marbles . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","when input is [1] the target: 1004\n","when input is [1, 1004] the target: 1005\n","when input is [1, 1004, 1005] the target: 1134\n","when input is [1, 1004, 1005, 1134] the target: 1007\n","when input is [1, 1004, 1005, 1134, 1007] the target: 8\n","when input is [1, 1004, 1005, 1134, 1007, 8] the target: 1161\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161] the target: 1136\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136] the target: 1018\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018] the target: 1049\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049] the target: 1136\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136] the target: 1137\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137] the target: 1016\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016] the target: 1044\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044] the target: 1137\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137] the target: 1007\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007] the target: 12\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12] the target: 1049\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049] the target: 1009\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009] the target: 1046\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046] the target: 1018\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018] the target: 1049\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049] the target: 1019\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019] the target: 1134\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134] the target: 1020\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020] the target: 1022\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022] the target: 1023\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023] the target: 1005\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005] the target: 1134\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134] the target: 1007\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007] the target: 36\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36] the target: 1049\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049] the target: 1016\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016] the target: 2\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n","when input is [1, 1004, 1005, 1134, 1007, 8, 1161, 1136, 1018, 1049, 1136, 1137, 1016, 1044, 1137, 1007, 12, 1049, 1009, 1046, 1018, 1049, 1019, 1134, 1020, 1022, 1023, 1005, 1134, 1007, 36, 1049, 1016, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] the target: 0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DXQMRPBVutWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# Model"],"metadata":{"id":"wC4s4-Dfuzhr"}},{"cell_type":"code","source":["#ref: https://pytorch.org/tutorials/beginner/translation_transformer.html\n","\n","from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.nn import Transformer\n","import copy\n","import math\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, emb_size, dropout, maxlen = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(0) #(b=1, maxlen=seq_len, emb_size=d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding):\n","        #token_embedding (b,t_src,c) / (b,t_tgt,c)\n","        seq_len = token_embedding.size(1)\n","        return self.dropout(token_embedding + self.pos_embedding[:,:seq_len]) #(b,t_src,c) + (b:1,t_src,c)\n","\n","# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        #tokens (b,t_src) / (b,t_tgt) dtype=torch.int64\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size) #self.embedding(tokens.long()) (b,t_src) / (b,t_tgt)\n","\n","def generate_square_subsequent_mask(size):\n","    #size = t_tgt\n","    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    #mask (t_tgt, t_tgt)\n","    return mask\n","\n","def create_mask(src, tgt):\n","    #src (b,t_src)\n","    #tgt (b,t_tgt)\n","    src_seq_len = src.shape[1]\n","    tgt_seq_len = tgt.shape[1]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len) #tgt_mask(t_tgt, t_tgt)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool) #src_mask(t_src,t_src)\n","\n","    src_padding_mask = (src == PAD_token) #src_padding_mask (b,t_src)\n","    tgt_padding_mask = (tgt == PAD_token) #tgt_padding_mask(b,t_tgt)\n","\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n","\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward, dropout,\n","                 activation = F.relu, layer_norm_eps = 1e-5):\n","        super().__init__()\n","        #from .activation import MultiheadAttention\n","        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n","        # Implementation of Feedforward model\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = activation\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","\n","        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n","        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, x, src_mask, src_key_padding_mask):\n","        #add first\n","        x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n","        x = self.norm2(x + self._ff_block(x))\n","\n","        \"\"\"\n","        norm first:\n","        x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n","        x = x + self._ff_block(self.norm2(x))\n","        \"\"\"\n","        return x\n","    # self-attention block\n","    def _sa_block(self, x, attn_mask, key_padding_mask):\n","        x = self.self_attn(x, x, x,\n","                           attn_mask=attn_mask,\n","                           key_padding_mask=key_padding_mask,\n","                           need_weights=False)[0]\n","        return self.dropout1(x)\n","\n","    # feed forward block\n","    def _ff_block(self, x):\n","        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n","        return self.dropout2(x)\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers, norm):\n","        super().__init__()\n","        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for i in range(num_layers)]) #nn.ModuleList([Block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n","        self.norm = norm\n","\n","    def forward(self, src, mask=None, src_key_padding_mask=None):\n","        output = src\n","        for mod in self.layers:\n","            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n","        output = self.norm(output) #paper dont have this\n","        return output\n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self, num_encoder_layers,\n","                 d_model, nhead, vocab_size,\n","                 dim_feedforward = 512, dropout = 0.1, activation = F.relu, layer_norm_eps = 1e-5):\n","        super().__init__()\n","        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps)\n","        encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n","        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n","\n","        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n","\n","        self.generator = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, src, src_mask=None, src_key_padding_mask=None): #memory mask is mask for cross-attention\n","        src_emb = self.positional_encoding(self.tok_emb(src))\n","\n","        memory = self.encoder(src_emb, src_mask, src_key_padding_mask)\n","        #memory = (b, t_src, c)\n","        logits = self.generator(memory)\n","        #logits = (b, t_tgt, vocab)\n","        return logits\n","\n","    \"\"\"def encode(self, src, src_mask):\n","        return self.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt, memory, tgt_mask):\n","        return self.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)\"\"\""],"metadata":{"id":"a0fhRr2Tuy_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB_SIZE = en.n_words\n","EMB_SIZE = 512 #for embedding & transformer\n","NHEAD = 8\n","FFN_HID_DIM = 512 #for fc layer\n","NUM_ENCODER_LAYERS = 3\n","PAD_token = en.word2index['<PAD>']\n","\n","model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, EMB_SIZE, NHEAD, VOCAB_SIZE, FFN_HID_DIM)\n","model = model.to(device)\n","\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_token)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"metadata":{"id":"hYggPitI4R3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BLOCK_SIZE = 100\n","MAX_TOKEN = 50\n","EOS_TOKEN = en.word2index['<EOS>']\n","\n","def sentenceFromIndexes(lang, idx):\n","    idx = idx.tolist()\n","    return \" \".join([lang.index2word[i] for i in idx])\n","\n","def respond(sentence, model):\n","    model.eval()\n","    idx = indexesFromSentence(en, sentence)\n","    idx = torch.tensor(idx).unsqueeze(0)\n","    q_len = idx.shape[1]\n","    idx = idx.to(device)\n","    \"\"\"print(f\"input {idx}\")\n","    print(sentenceFromIndexes(en, idx[0]))\"\"\"\n","    with torch.no_grad():\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(MAX_TOKEN):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -BLOCK_SIZE:]\n","            # get the predictions\n","            logits = model(idx_cond, None, None) #logits=(b,t,c)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","\n","            if idx_next == EOS_TOKEN:\n","                break\n","    return sentenceFromIndexes(en, idx[0, q_len:])\n"],"metadata":{"id":"Cps-hxp1IaDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","question = random.choice(questions)\n","print(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHrzLRFlIcRO","executionInfo":{"status":"ok","timestamp":1689151590673,"user_tz":-480,"elapsed":384,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"83d71286-fd06-45c6-f591-1bae224f4ada"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Q: Alex has 5 times as many marbles as Lily. If Lily has 10 marbles, how many marbles does Alex have?\n","\n"]}]},{"cell_type":"code","source":["num_epoch = 5000\n","eval_interval = 100\n","eval_iters = 200\n","\n","train_dataloader = get_dataloader(64)\n","print(question)\n","for iter in range(num_epoch):\n","    model.train()\n","    losses = 0\n","    count = 0\n","\n","    \"\"\"# every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1 or iter == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\"\"\"\n","\n","    # sample a batch of data\n","    for x, Y in train_dataloader:\n","        x, Y = x.to(device), Y.to(device)\n","        #x, Y (b,t)\n","\n","        src_mask, tgt_mask, src_key_padding_mask, tgt_padding_mask = create_mask(x, x)\n","\n","        y_pred = model(x, tgt_mask, src_key_padding_mask)\n","        #y_pred = (b,t,vocab_size)\n","        optimizer.zero_grad()\n","\n","        loss = loss_fn(y_pred.reshape(-1, y_pred.shape[-1]), Y.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","        count += 1\n","\n","    print(f\"Epoch: {iter+1} \\t Train Loss: {(losses/count):.4f} \\t{respond(question, model)}\")\n"],"metadata":{"id":"elULq9eR4R92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model, \"myGPT.pt\")"],"metadata":{"id":"eXcNwdyJdOcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","    question = random.choice(questions)\n","    print(f\"Question {i+1}, {question}\")\n","    print(f\"Generated Answer, {respond(question, model)}\")\n","    print(\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKoMryDBDPnD","executionInfo":{"status":"ok","timestamp":1689154532403,"user_tz":-480,"elapsed":1663,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"66087224-085f-4461-e629-602bfe41bda8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question 1, Q: A class has 30 students. If 1/4 of them are absent, how many students are present?\n","Generated Answer, A : There are 22 students present . <EOS>\n","\n","Question 2, Q: A class has 35 students. If 3/7 of them are absent, how many students are present?\n","Generated Answer, A : There are 20 students absent . <EOS>\n","\n","Question 3, Q: A recipe requires 3/4 cup of sugar. If you want to make 2 recipes, how many cups of sugar do you need?\n","Generated Answer, A : You need 1 . 5 cups of sugar . <EOS>\n","\n","Question 4, Q: A school has 600 students. If 3/5 of the students are girls, how many boys are there in the school?\n","Generated Answer, A : There are 240 boys in the school . <EOS>\n","\n","Question 5, Q: A car travels at an average speed of 70 kilometers per hour. How long will it take to travel a distance of 140 kilometers?\n","Generated Answer, A : It will take 2 hours . <EOS>\n","\n","Question 6, Q: A store sells apples for $0.50 each and bananas for $0.25 each. If Lisa buys 6 apples and 8 bananas, how much does she spend?\n","Generated Answer, A : Lisa spends $ 4 . <EOS>\n","\n","Question 7, Q: Peter has 3 times as many marbles as Jack. If Jack has 8 marbles, how many marbles does Peter have?\n","Generated Answer, A : Peter has 24 marbles . <EOS>\n","\n","Question 8, Q: A rectangular field has a length of 20 meters and a width of 10 meters. What is the area of the field?\n","Generated Answer, A : The area of the field is 200 square meters . <EOS>\n","\n","Question 9, Q: A recipe requires 3/4 cup of flour. If you want to make 2 recipes, how many cups of flour do you need?\n","Generated Answer, A : You need 1 . 5 cups of flour . <EOS>\n","\n","Question 10, Q: A rectangular garden has a length of 10 meters and a width of 5 meters. What is the area of the garden?\n","Generated Answer, A : The area of the garden is 50 square meters . <EOS>\n","\n"]}]},{"cell_type":"code","source":["q = \"Peter has 3 times as many car as Jack. If Jack has 8 car, how many marbles does Peter have?\"\n","print(respond(q, model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bbd3jEEwKYb","executionInfo":{"status":"ok","timestamp":1689155025224,"user_tz":-480,"elapsed":546,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"e358cbbf-5975-4cfe-95b5-f158f7b0b403"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A : Peter has 32 marbles . <EOS>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eXCTgdM0XNDk"},"execution_count":null,"outputs":[]}]}